{
    "model": "rnn",
    "tokenizer": "vec",
    "m": 100,
    "learning_rate": 0.001,
    "seq_length": 25,
    "batch_size": 1,
    "gradient_clip": 5,
    "smooth_loss_factor": 0.999,
    "n_iters": 100000,
    "log_every": 1000,
    "syntesize_every": 10000,
    "train_size": 0.9, 
    "eval_iters": 100,
    "sampling": "nucleus",
    "temperature": 0.9,
    "nucleus": 0.9,
    "max_new_tokens": 200
  }
  